{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Jupyter notebook extracts the most informative features i.e. words from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Text Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yeah I got 2 backups for all that. I just hate...</td>\n",
       "      <td>Non-Bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate using my BB  but love my iPhone. Haven'...</td>\n",
       "      <td>Non-Bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>Bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>Bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>Bullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet    Text Label\n",
       "0  yeah I got 2 backups for all that. I just hate...  Non-Bullying\n",
       "1  I hate using my BB  but love my iPhone. Haven'...  Non-Bullying\n",
       "2                             Get fucking real dude.      Bullying\n",
       "3   She is as dirty as they come  and that crook ...      Bullying\n",
       "4   why did you fuck it up. I could do it all day...      Bullying"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet = []\n",
    "Labels = []\n",
    "\n",
    "for row in df[\"Tweet\"]:\n",
    "    #tokenize words\n",
    "    words = word_tokenize(row)\n",
    "    #remove punctuations\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    #remove stop words\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\" ]\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "    #Lematise words\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "    Tweet.append(lemma_list)\n",
    "\n",
    "    for row in df[\"Text Label\"]:\n",
    "        Labels.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(Tweet, Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new list for modeling\n",
    "Final_Data = []\n",
    "for r, v in combined:\n",
    "    bag_of_words(r)\n",
    "    Final_Data.append((bag_of_words(r),v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance with Unigrams \n",
      "Accuracy: 0.6662379421221865\n"
     ]
    }
   ],
   "source": [
    "#Split the data into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set= train_test_split(Final_Data, test_size=0.15, random_state=42)\n",
    "\n",
    "#Naive Bayes for Unigramsm check accuracy\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "print(\"Naive Bayes Performance with Unigrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  fucker = True           Bullyi : Non-Bu =     11.7 : 1.0\n",
      "                    liar = True           Bullyi : Non-Bu =      7.6 : 1.0\n",
      "                  vagina = True           Bullyi : Non-Bu =      7.6 : 1.0\n",
      "                      iq = True           Bullyi : Non-Bu =      7.0 : 1.0\n",
      "                     low = True           Bullyi : Non-Bu =      6.8 : 1.0\n",
      "                   worth = True           Non-Bu : Bullyi =      6.6 : 1.0\n",
      "                  douche = True           Bullyi : Non-Bu =      6.2 : 1.0\n",
      "                 violent = True           Bullyi : Non-Bu =      6.2 : 1.0\n",
      "                    tire = True           Bullyi : Non-Bu =      6.2 : 1.0\n",
      "                     non = True           Bullyi : Non-Bu =      6.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Find most informative features\n",
    "classifier.show_most_informative_features(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same thing with Bigrams\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(Tweet,Labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words of Bigrams\n",
    "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)  \n",
    "    bigrams = bigram_finder.nbest(score_fn, n)  \n",
    "    return bag_of_words(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data2 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_bigrams_words(z)\n",
    "    Final_Data2.append((bag_of_bigrams_words(z),e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance with Bigrams \n",
      "Accuracy: 0.6164978292329957\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set= train_test_split(Final_Data2, test_size=0.2, random_state=42)\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "\n",
    "#Naive Bayes for Bigrams\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Performance with Bigrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           ('low', 'iq') = True           Bullyi : Non-Bu =     15.2 : 1.0\n",
      "       ('piece', 'shit') = True           Bullyi : Non-Bu =     12.0 : 1.0\n",
      "        ('time', 'http') = True           Bullyi : Non-Bu =      7.6 : 1.0\n",
      "           ('fuck', 'u') = True           Bullyi : Non-Bu =      7.6 : 1.0\n",
      "       ('damn', 'homie') = True           Bullyi : Non-Bu =      7.6 : 1.0\n",
      "        ('suck', 'dick') = True           Bullyi : Non-Bu =      7.6 : 1.0\n",
      "        ('shut', 'fuck') = True           Bullyi : Non-Bu =      6.8 : 1.0\n",
      "     ('fucking', 'cunt') = True           Bullyi : Non-Bu =      6.2 : 1.0\n",
      "        ('holy', 'shit') = True           Bullyi : Non-Bu =      6.2 : 1.0\n",
      "          ('gay', 'guy') = True           Bullyi : Non-Bu =      5.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(Tweet,Labels)\n",
    "#Same thing with Trigrams\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq, n=200):\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(words)  \n",
    "    trigrams = trigram_finder.nbest(score_fn, n)  \n",
    "    return bag_of_words(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data3 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_trigrams_words(z)\n",
    "    Final_Data3.append((bag_of_trigrams_words(z),e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance with Trigrams \n",
      "Accuracy: 0.670096463022508\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set= train_test_split(Final_Data3, test_size=0.15, random_state=42)\n",
    "\n",
    "#Naive Bayes for Trigrams\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Performance with Trigrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "('worthless', 'piece', 'shit') = True           Bullyi : Non-Bu =      7.0 : 1.0\n",
      "   ('oh', 'man', 'suck') = True           Bullyi : Non-Bu =      3.4 : 1.0\n",
      "('pretty', 'damn', 'awesome') = True           Bullyi : Non-Bu =      3.4 : 1.0\n",
      "   ('got', 'ta', 'hate') = True           Bullyi : Non-Bu =      3.4 : 1.0\n",
      "   ('aw', 'man', 'suck') = True           Bullyi : Non-Bu =      3.4 : 1.0\n",
      "   ('wan', 'na', 'fuck') = True           Bullyi : Non-Bu =      2.9 : 1.0\n",
      " ('suck', 'big', 'time') = True           Bullyi : Non-Bu =      2.9 : 1.0\n",
      "    (\"'re\", 'gon', 'na') = True           Non-Bu : Bullyi =      2.7 : 1.0\n",
      "    ('gon', 'na', 'get') = True           Non-Bu : Bullyi =      2.4 : 1.0\n",
      "('happy', 'new', 'year') = True           Non-Bu : Bullyi =      2.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(Tweet,Labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine both unigrams, bigrams, and trigrams\n",
    "\n",
    "# Import Bigram metrics - we will use these to identify the top 200 bigrams\n",
    "def bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq,\n",
    "n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return bigrams\n",
    "\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "\n",
    "# Import Bigram metrics - we will use these to identify the top 200 trigrams \n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "def trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq,\n",
    "n=200):\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(words)\n",
    "    trigrams = trigram_finder.nbest(score_fn, n)\n",
    "    return trigrams\n",
    "\n",
    "#bag of ngrams\n",
    "def bag_of_Ngrams_words(words):\n",
    "    bigramBag = bigrams_words(words)\n",
    "    \n",
    "    #The following two for loops convert tuple into string\n",
    "    for b in range(0,len(bigramBag)):\n",
    "        bigramBag[b]=' '.join(bigramBag[b])\n",
    "   \n",
    "    trigramBag = trigrams_words(words)\n",
    "    for t in range(0,len(trigramBag)):\n",
    "        trigramBag[t]=' '.join(trigramBag[t])\n",
    "\n",
    "    return bag_of_words(trigramBag + bigramBag + words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data4 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_Ngrams_words(z)\n",
    "    Final_Data4.append((bag_of_Ngrams_words(z),e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance with Ngrams \n",
      "Accuracy: 0.670096463022508\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set= train_test_split(Final_Data3, test_size=0.15, random_state=42)\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "#Naive Bayes for Ngrams\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Performance with Ngrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "('worthless', 'piece', 'shit') = True           Bullyi : Non-Bu =      7.0 : 1.0\n",
      "   ('oh', 'man', 'suck') = True           Bullyi : Non-Bu =      3.4 : 1.0\n",
      "('pretty', 'damn', 'awesome') = True           Bullyi : Non-Bu =      3.4 : 1.0\n",
      "   ('got', 'ta', 'hate') = True           Bullyi : Non-Bu =      3.4 : 1.0\n",
      "   ('aw', 'man', 'suck') = True           Bullyi : Non-Bu =      3.4 : 1.0\n",
      "   ('wan', 'na', 'fuck') = True           Bullyi : Non-Bu =      2.9 : 1.0\n",
      " ('suck', 'big', 'time') = True           Bullyi : Non-Bu =      2.9 : 1.0\n",
      "    (\"'re\", 'gon', 'na') = True           Non-Bu : Bullyi =      2.7 : 1.0\n",
      "    ('gon', 'na', 'get') = True           Non-Bu : Bullyi =      2.4 : 1.0\n",
      "('happy', 'new', 'year') = True           Non-Bu : Bullyi =      2.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
